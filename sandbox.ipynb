{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing area\n",
    "\n",
    "## Step 1: Data Collection\n",
    "First, gather the stock price history, trading volume, and other relevant indicators. Use the finvizfinance package to get stock data and a package like yahoo_fin for news. Ensure you have these packages installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "#%pip install finvizfinance yahoo-fin tensorflow pandas numpy scikit-learn requests_html tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning - Certain functionality \n",
      "             requires requests_html, which is not installed.\n",
      "             \n",
      "             Install using: \n",
      "             pip install requests_html\n",
      "             \n",
      "             After installation, you may have to restart your Python session.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from finvizfinance.quote import finvizfinance\n",
    "from yahoo_fin import stock_info as si\n",
    "from yahoo_fin import news\n",
    "from datetime import datetime, timedelta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Gather Stock Data\n",
    "Fetch the stock price history and other indicators, like trading volume, for a specific ticker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a stock ticker symbol\n",
    "ticker_symbol = \"AAPL\"\n",
    "\n",
    "# Get stock price history\n",
    "stock_data = si.get_data(ticker_symbol, start_date=\"2022-01-01\")\n",
    "\n",
    "# Include relevant features\n",
    "stock_data[\"trading_volume\"] = stock_data[\"volume\"]\n",
    "stock_data[\"price_change\"] = stock_data[\"close\"].diff()  # Daily price change\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Fetch News and Analyze Sentiment\n",
    "Fetch news articles related to the ticker and analyze their sentiment. This step might require additional sentiment analysis tools or pre-trained sentiment models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf-keras\n",
      "  Downloading tf_keras-2.16.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: tensorflow<2.17,>=2.16 in c:\\users\\oskar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tf-keras) (2.16.1)\n",
      "Requirement already satisfied: tensorflow-intel==2.16.1 in c:\\users\\oskar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow<2.17,>=2.16->tf-keras) (2.16.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\oskar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\oskar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\oskar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\oskar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\oskar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\oskar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\oskar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in c:\\users\\oskar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\oskar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\oskar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (24.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\oskar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\oskar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (2.31.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\oskar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (69.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\oskar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\oskar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\oskar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\oskar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\oskar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (1.63.0)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in c:\\users\\oskar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in c:\\users\\oskar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (3.3.3)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in c:\\users\\oskar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\oskar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (0.43.0)\n",
      "Requirement already satisfied: rich in c:\\users\\oskar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\oskar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\oskar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (0.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\oskar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\oskar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\oskar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\oskar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (2024.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\oskar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\oskar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\oskar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\oskar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\oskar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\oskar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\oskar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (0.1.2)\n",
      "Downloading tf_keras-2.16.0-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.0/1.7 MB 495.5 kB/s eta 0:00:04\n",
      "   --- ------------------------------------ 0.2/1.7 MB 1.4 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 0.4/1.7 MB 2.6 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 0.6/1.7 MB 3.0 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 0.9/1.7 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.1/1.7 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.3/1.7 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.5/1.7 MB 4.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.5/1.7 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.7/1.7 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 3.4 MB/s eta 0:00:00\n",
      "Installing collected packages: tf-keras\n",
      "Successfully installed tf-keras-2.16.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Oskar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from goose3 import Goose                             # type: ignore\n",
    "from transformers import AutoTokenizer               # type: ignore\n",
    "from nltk.tokenize import sent_tokenize              # type: ignore\n",
    "from bs4 import BeautifulSoup                        # type: ignore\n",
    "from transformers import pipeline                    # type: ignore\n",
    "import yfinance as yf                                # type: ignore\n",
    "from requests import get                             # type: ignore\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# For simplicity, let's assume a basic sentiment analysis function\n",
    "# Here, you might use a pre-trained sentiment analysis model to determine sentiment\n",
    "# This is a placeholder for your sentiment analysis logic\n",
    "def get_ticker_news_sentiment(ticker):\n",
    "\n",
    "    ALLOW_TOKENIZATION = False # True: the model will feed the article into the model in chunks of 512 tokens, \n",
    "    #                            False: the model will consider only the first sentences of the article until the total number of tokens does not exceed 512\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "\n",
    "    ticker_news = yf.Ticker(ticker)\n",
    "    try:\n",
    "        news_list = ticker_news.get_news()\n",
    "    except:\n",
    "        print(f\"Error getting news for ticker {ticker}\")\n",
    "        return\n",
    "    \n",
    "    extractor = Goose()\n",
    "    pipe = pipeline(\"text-classification\", model=\"ProsusAI/finbert\")\n",
    "    data = []\n",
    "    \n",
    "    headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "    }\n",
    "                \n",
    "    for dic in news_list:\n",
    "        title = dic['title']\n",
    "        response = get(dic['link'], headers=headers)\n",
    "        \n",
    "        article = extractor.extract(raw_html=response.content)\n",
    "        text = article.cleaned_text\n",
    "        date = article.publish_date\n",
    "        \n",
    "        if date == None: # If the date is not found in the article, try to find it in the article's html\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Yahoo Finance usually stores the publication date in a 'time' tag with the class 'caas-attr-meta-time'\n",
    "            date_tag = soup.find('time', {'class': 'caas-attr-meta-time'})\n",
    "            if date_tag:\n",
    "                date = date_tag['datetime']\n",
    "            else:\n",
    "                print('Publication date not found, article link for debugging', dic['link'])\n",
    "            \n",
    "            \n",
    "        if len(text) > 512:\n",
    "            if ALLOW_TOKENIZATION: # feed the article into the model in chunks of 512 tokens\n",
    "                inputs = tokenizer.encode_plus(\n",
    "                    text,\n",
    "                    max_length=510,\n",
    "                    truncation='longest_first',  # Truncate the longest sequences first\n",
    "                    padding='max_length',  # Pad sequences to the max length\n",
    "                    return_tensors='pt',  # Return PyTorch tensors\n",
    "                )\n",
    "                \n",
    "                # Convert tensor to list and then to string\n",
    "                input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "                new_text = tokenizer.decode(input_ids)\n",
    "            \n",
    "            else: # count the sentences until the total number of tokens does not exceed 512 (consider only first sentences of the article)\n",
    "                # Split the text into sentences\n",
    "                sentences = sent_tokenize(text)\n",
    "\n",
    "                # Initialize an empty string for the new text\n",
    "                new_text = ''\n",
    "\n",
    "                # Add sentences to the new text until it exceeds 512 tokens\n",
    "                for sentence in sentences:\n",
    "                    if len(new_text.split()) + len(sentence) > 512:\n",
    "                        new_text += ' ' + sentence\n",
    "                    break\n",
    "\n",
    "            # Now you can pass 'inputs' to your model\n",
    "            results = pipe(new_text)\n",
    "\n",
    "            data.append({'Ticker':f'{ticker}',\n",
    "                         'Date':f'{date}',\n",
    "                         'Article title':f'{title}',\n",
    "                         'Article sentiment':results[0]['label']})\n",
    "\n",
    "        else:\n",
    "            results = pipe(text)\n",
    "            data.append({'Ticker':f'{ticker}',\n",
    "                         'Date':f'{date}',\n",
    "                         'Article title':f'{title}',\n",
    "                         'Article sentiment':results[0]['label']})\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "\n",
    "news_df_orig = get_ticker_news_sentiment(ticker_symbol)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>News_timestamp</th>\n",
       "      <th>News_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-05-09T08:45:00.000Z</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-05-09T05:00:00.000Z</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-05-09T03:45:43.000Z</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-05-09T03:05:00.000Z</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-05-08T23:47:00.000Z</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2024-05-08T21:49:00.000Z</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2024-05-08T21:39:49.000Z</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2024-05-08T21:20:50.000Z</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             News_timestamp  News_sentiment\n",
       "0  2024-05-09T08:45:00.000Z               0\n",
       "1  2024-05-09T05:00:00.000Z               0\n",
       "2  2024-05-09T03:45:43.000Z               0\n",
       "3  2024-05-09T03:05:00.000Z              -1\n",
       "4  2024-05-08T23:47:00.000Z               0\n",
       "5  2024-05-08T21:49:00.000Z               0\n",
       "6  2024-05-08T21:39:49.000Z               0\n",
       "7  2024-05-08T21:20:50.000Z               0"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Align News with Stock Data\n",
    "\n",
    "Align the news sentiment data with the stock price history to create a unified dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adjclose</th>\n",
       "      <th>volume</th>\n",
       "      <th>ticker</th>\n",
       "      <th>trading_volume</th>\n",
       "      <th>price_change</th>\n",
       "      <th>Article timestamp</th>\n",
       "      <th>Article sentiment</th>\n",
       "      <th>MA_10</th>\n",
       "      <th>MA_200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-01-03</th>\n",
       "      <td>177.830002</td>\n",
       "      <td>182.880005</td>\n",
       "      <td>177.710007</td>\n",
       "      <td>182.009995</td>\n",
       "      <td>179.724548</td>\n",
       "      <td>104487900</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>104487900</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-04</th>\n",
       "      <td>182.630005</td>\n",
       "      <td>182.940002</td>\n",
       "      <td>179.119995</td>\n",
       "      <td>179.699997</td>\n",
       "      <td>177.443573</td>\n",
       "      <td>99310400</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>99310400</td>\n",
       "      <td>-2.309998</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-05</th>\n",
       "      <td>179.610001</td>\n",
       "      <td>180.169998</td>\n",
       "      <td>174.639999</td>\n",
       "      <td>174.919998</td>\n",
       "      <td>172.723587</td>\n",
       "      <td>94537600</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>94537600</td>\n",
       "      <td>-4.779999</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-06</th>\n",
       "      <td>172.699997</td>\n",
       "      <td>175.300003</td>\n",
       "      <td>171.639999</td>\n",
       "      <td>172.000000</td>\n",
       "      <td>169.840256</td>\n",
       "      <td>96904000</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>96904000</td>\n",
       "      <td>-2.919998</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-07</th>\n",
       "      <td>172.889999</td>\n",
       "      <td>174.139999</td>\n",
       "      <td>171.029999</td>\n",
       "      <td>172.169998</td>\n",
       "      <td>170.008102</td>\n",
       "      <td>86709100</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>86709100</td>\n",
       "      <td>0.169998</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-05-02</th>\n",
       "      <td>172.509995</td>\n",
       "      <td>173.419998</td>\n",
       "      <td>170.889999</td>\n",
       "      <td>173.029999</td>\n",
       "      <td>173.029999</td>\n",
       "      <td>94214900</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>94214900</td>\n",
       "      <td>3.729996</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>169.211000</td>\n",
       "      <td>181.24575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-05-03</th>\n",
       "      <td>186.649994</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>182.660004</td>\n",
       "      <td>183.380005</td>\n",
       "      <td>183.380005</td>\n",
       "      <td>163224100</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>163224100</td>\n",
       "      <td>10.350006</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>171.049001</td>\n",
       "      <td>181.18715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-05-06</th>\n",
       "      <td>182.350006</td>\n",
       "      <td>184.199997</td>\n",
       "      <td>180.419998</td>\n",
       "      <td>181.710007</td>\n",
       "      <td>181.710007</td>\n",
       "      <td>78569700</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>78569700</td>\n",
       "      <td>-1.669998</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>172.636002</td>\n",
       "      <td>181.13005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-05-07</th>\n",
       "      <td>183.449997</td>\n",
       "      <td>184.899994</td>\n",
       "      <td>181.320007</td>\n",
       "      <td>182.399994</td>\n",
       "      <td>182.399994</td>\n",
       "      <td>77305800</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>77305800</td>\n",
       "      <td>0.689987</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>174.186002</td>\n",
       "      <td>181.08235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-05-08</th>\n",
       "      <td>182.850006</td>\n",
       "      <td>183.070007</td>\n",
       "      <td>181.449997</td>\n",
       "      <td>182.740005</td>\n",
       "      <td>182.740005</td>\n",
       "      <td>45033300</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>45033300</td>\n",
       "      <td>0.340012</td>\n",
       "      <td>2024-05-08 22:09:09.750000128+00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>175.558002</td>\n",
       "      <td>181.03230</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>590 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  open        high         low       close    adjclose  \\\n",
       "2022-01-03  177.830002  182.880005  177.710007  182.009995  179.724548   \n",
       "2022-01-04  182.630005  182.940002  179.119995  179.699997  177.443573   \n",
       "2022-01-05  179.610001  180.169998  174.639999  174.919998  172.723587   \n",
       "2022-01-06  172.699997  175.300003  171.639999  172.000000  169.840256   \n",
       "2022-01-07  172.889999  174.139999  171.029999  172.169998  170.008102   \n",
       "...                ...         ...         ...         ...         ...   \n",
       "2024-05-02  172.509995  173.419998  170.889999  173.029999  173.029999   \n",
       "2024-05-03  186.649994  187.000000  182.660004  183.380005  183.380005   \n",
       "2024-05-06  182.350006  184.199997  180.419998  181.710007  181.710007   \n",
       "2024-05-07  183.449997  184.899994  181.320007  182.399994  182.399994   \n",
       "2024-05-08  182.850006  183.070007  181.449997  182.740005  182.740005   \n",
       "\n",
       "               volume ticker  trading_volume  price_change  \\\n",
       "2022-01-03  104487900   AAPL       104487900           NaN   \n",
       "2022-01-04   99310400   AAPL        99310400     -2.309998   \n",
       "2022-01-05   94537600   AAPL        94537600     -4.779999   \n",
       "2022-01-06   96904000   AAPL        96904000     -2.919998   \n",
       "2022-01-07   86709100   AAPL        86709100      0.169998   \n",
       "...               ...    ...             ...           ...   \n",
       "2024-05-02   94214900   AAPL        94214900      3.729996   \n",
       "2024-05-03  163224100   AAPL       163224100     10.350006   \n",
       "2024-05-06   78569700   AAPL        78569700     -1.669998   \n",
       "2024-05-07   77305800   AAPL        77305800      0.689987   \n",
       "2024-05-08   45033300   AAPL        45033300      0.340012   \n",
       "\n",
       "                             Article timestamp  Article sentiment       MA_10  \\\n",
       "2022-01-03                                 NaT                0.0         NaN   \n",
       "2022-01-04                                 NaT                0.0         NaN   \n",
       "2022-01-05                                 NaT                0.0         NaN   \n",
       "2022-01-06                                 NaT                0.0         NaN   \n",
       "2022-01-07                                 NaT                0.0         NaN   \n",
       "...                                        ...                ...         ...   \n",
       "2024-05-02                                 NaT                0.0  169.211000   \n",
       "2024-05-03                                 NaT                0.0  171.049001   \n",
       "2024-05-06                                 NaT                0.0  172.636002   \n",
       "2024-05-07                                 NaT                0.0  174.186002   \n",
       "2024-05-08 2024-05-08 22:09:09.750000128+00:00                0.0  175.558002   \n",
       "\n",
       "               MA_200  \n",
       "2022-01-03        NaN  \n",
       "2022-01-04        NaN  \n",
       "2022-01-05        NaN  \n",
       "2022-01-06        NaN  \n",
       "2022-01-07        NaN  \n",
       "...               ...  \n",
       "2024-05-02  181.24575  \n",
       "2024-05-03  181.18715  \n",
       "2024-05-06  181.13005  \n",
       "2024-05-07  181.08235  \n",
       "2024-05-08  181.03230  \n",
       "\n",
       "[590 rows x 13 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a copy of the stock data and news data (to avoid modifying/rerunning the code to get the original data)\n",
    "stock_data_copy = stock_data.copy()\n",
    "news_df_copy = news_df_orig.copy()\n",
    "\n",
    "# Some data cleaning and preprocessing\n",
    "news_df_copy = news_df_copy.rename(columns={\n",
    "    'Ticker': 'Ticker',\n",
    "    'Date': 'Article timestamp',\n",
    "    'Article title': 'Article title',\n",
    "})\n",
    "\n",
    "sentiment_mapping = {\n",
    "    'positive': 1,\n",
    "    'neutral': 0,\n",
    "    'negative': -1\n",
    "}\n",
    "\n",
    "news_df_copy['Article sentiment'] = news_df_copy['Article sentiment'].map(sentiment_mapping)\n",
    "news_df_copy = news_df_copy.drop(columns=['Ticker', 'Article title']) # Drop unnecessary columns\n",
    "\n",
    "# Convert news timestamps to datetime\n",
    "news_df_copy[\"Article timestamp\"] = pd.to_datetime(news_df_copy[\"Article timestamp\"])\n",
    "news_df_copy[\"date\"] = news_df_copy[\"Article timestamp\"].dt.date\n",
    "\n",
    "# Convert 'date' column to a DatetimeIndex\n",
    "news_df_copy['date'] = pd.to_datetime(news_df_copy['date'])\n",
    "\n",
    "# Set 'date' as the index\n",
    "news_df_copy.set_index('date', inplace=True)\n",
    "\n",
    "# Now resample news_df\n",
    "news_daily_sentiment = news_df_copy.resample(\"D\").mean()\n",
    "\n",
    "\n",
    "# Merge stock data with daily news sentiment\n",
    "merged_data = stock_data_copy.join(news_daily_sentiment, how='left')\n",
    "\n",
    "# Fill NaNs with neutral sentiment (0) if there are no news articles for a given day\n",
    "merged_data[\"Article sentiment\"] = merged_data[\"Article sentiment\"].fillna(0)\n",
    "\n",
    "    # Feature engineering: additional features like moving averages\n",
    "if \"MA_10\" not in merged_data.columns: # Run this block only once\n",
    "    merged_data[\"MA_10\"] = merged_data[\"close\"].rolling(window=10).mean()  # 10-day moving average\n",
    "    merged_data[\"MA_200\"] = merged_data[\"close\"].rolling(window=200).mean()  # 200-day moving average (not used yet)\n",
    "\n",
    "merged_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Build and Train the Neural Network\n",
    "Create and train the neural network to predict future stock prices based on the collected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(580,)\n",
      "(580, 4)\n",
      "WARNING:tensorflow:From c:\\Users\\Oskar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Oskar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\optimizers\\__init__.py:317: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From c:\\Users\\Oskar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\autograph\\converters\\directives.py:126: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "15/15 [==============================] - 25s 53ms/step - loss: 27991.9668 - val_loss: 26294.5781\n",
      "Epoch 2/10\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 27717.5762 - val_loss: 25981.5293\n",
      "Epoch 3/10\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 27305.4512 - val_loss: 25486.0293\n",
      "Epoch 4/10\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 26652.9844 - val_loss: 24705.0430\n",
      "Epoch 5/10\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 25618.0059 - val_loss: 23505.1699\n",
      "Epoch 6/10\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 24107.1270 - val_loss: 21806.8125\n",
      "Epoch 7/10\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 22029.0352 - val_loss: 19574.8535\n",
      "Epoch 8/10\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 19303.3262 - val_loss: 16832.7617\n",
      "Epoch 9/10\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 16046.4062 - val_loss: 13708.2070\n",
      "Epoch 10/10\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 12778.7598 - val_loss: 10546.6113\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x259695ce510>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "train_data = merged_data.copy()\n",
    "\n",
    "# Define features and targets\n",
    "features = train_data[[\"close\", \"trading_volume\", \"Article sentiment\", \"MA_10\"]]\n",
    "target = train_data[\"close\"].shift(-1)  # Predict the next day's closing price\n",
    "\n",
    "# Combine features and target into one DataFrame\n",
    "data = features.copy()\n",
    "data['target'] = target\n",
    "\n",
    "# Drop rows with NaN values\n",
    "data.dropna(subset=['MA_10', 'target'], inplace=True)\n",
    "\n",
    "# Separate features and target\n",
    "features = data.drop(columns='target')\n",
    "target = data['target']\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(features_scaled, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the neural network architecture\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(64, activation=\"relu\", input_shape=(4,)),  # 4 input features\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(64, activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(1)  # Output layer with one node (regression)\n",
    "])\n",
    "\n",
    "# Compile the model with an optimizer and a loss function\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "# Train the model with the training set and validate with the validation set\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Implement Online Learning (Continuous Model Updates)\n",
    "To implement a form of online learning, you need to define a process to update the model with new incoming data, thus allowing the model to continuously learn and adapt to changing market conditions.\n",
    "\n",
    "Collect New Data: Fetch recent stock data and news articles. Align the news sentiment with stock prices.\n",
    "Feature Engineering and Normalization: Ensure new data is prepared similarly to the original training data, including feature normalization.\n",
    "Update the Model: Use the new data to continue training the model, simulating online learning. This may involve retraining the model for a few epochs with recent data.\n",
    "Performance Monitoring: Track key metrics like MSE and Mean Absolute Error (MAE) to detect model drift. Define thresholds to trigger retraining when necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from yahoo_fin import news\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Backtesting and Evaluation\n",
    "Before deploying the model in a real-world scenario, it's crucial to evaluate its performance using backtesting. This process involves simulating how the model would perform in historical scenarios to ensure accuracy and stability.\n",
    "\n",
    "This step calculates Mean Squared Error (MSE) and Mean Absolute Error (MAE) to evaluate how well the model predicts future stock prices. Consider expanding the metrics to evaluate different aspects of the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  open        high         low       close    adjclose  \\\n",
      "2023-01-03  130.279999  130.899994  124.169998  125.070000  124.216301   \n",
      "2023-01-04  126.889999  128.660004  125.080002  126.360001  125.497498   \n",
      "2023-01-05  127.129997  127.769997  124.760002  125.019997  124.166641   \n",
      "2023-01-06  126.010002  130.289993  124.889999  129.619995  128.735214   \n",
      "2023-01-09  130.470001  133.410004  129.889999  130.149994  129.261612   \n",
      "...                ...         ...         ...         ...         ...   \n",
      "2023-12-22  195.179993  195.410004  192.970001  193.600006  193.353287   \n",
      "2023-12-26  193.610001  193.889999  192.830002  193.050003  192.803986   \n",
      "2023-12-27  192.490005  193.500000  191.089996  193.149994  192.903839   \n",
      "2023-12-28  194.139999  194.660004  193.169998  193.580002  193.333298   \n",
      "2023-12-29  193.899994  194.399994  191.729996  192.529999  192.284637   \n",
      "\n",
      "               volume ticker  future_stock_price       MA_10  \\\n",
      "2023-01-03  112117500   AAPL          126.360001         NaN   \n",
      "2023-01-04   89113600   AAPL          125.019997         NaN   \n",
      "2023-01-05   80962700   AAPL          129.619995         NaN   \n",
      "2023-01-06   87754700   AAPL          130.149994         NaN   \n",
      "2023-01-09   70790800   AAPL          130.729996         NaN   \n",
      "...               ...    ...                 ...         ...   \n",
      "2023-12-22   37122800   AAPL          193.050003  195.747002   \n",
      "2023-12-26   28919300   AAPL          193.149994  195.734003   \n",
      "2023-12-27   48087700   AAPL          193.580002  195.578001   \n",
      "2023-12-28   34049900   AAPL          192.529999  195.140001   \n",
      "2023-12-29   42628800   AAPL                 NaN  194.582001   \n",
      "\n",
      "            Article sentiment  \n",
      "2023-01-03                  0  \n",
      "2023-01-04                  0  \n",
      "2023-01-05                  0  \n",
      "2023-01-06                  0  \n",
      "2023-01-09                  0  \n",
      "...                       ...  \n",
      "2023-12-22                  0  \n",
      "2023-12-26                  0  \n",
      "2023-12-27                  0  \n",
      "2023-12-28                  0  \n",
      "2023-12-29                  0  \n",
      "\n",
      "[250 rows x 10 columns]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- volume\nFeature names seen at fit time, yet now missing:\n- close\n- trading_volume\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[121], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m backtest_target \u001b[38;5;241m=\u001b[39m backtest_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfuture_stock_price\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdropna()\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Normalize backtesting features\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m backtest_features_scaled \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbacktest_features\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Oskar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    301\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\Oskar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:1043\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[1;34m(self, X, copy)\u001b[0m\n\u001b[0;32m   1040\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1042\u001b[0m copy \u001b[38;5;241m=\u001b[39m copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[1;32m-> 1043\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1044\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1045\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1046\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1047\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1048\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1049\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1050\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1052\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[0;32m   1053\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_mean:\n",
      "File \u001b[1;32mc:\\Users\\Oskar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:608\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_data\u001b[39m(\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    539\u001b[0m     X\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    544\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params,\n\u001b[0;32m    545\u001b[0m ):\n\u001b[0;32m    546\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate input data and set or check the `n_features_in_` attribute.\u001b[39;00m\n\u001b[0;32m    547\u001b[0m \n\u001b[0;32m    548\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    606\u001b[0m \u001b[38;5;124;03m        validated.\u001b[39;00m\n\u001b[0;32m    607\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 608\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_feature_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    610\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tags()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_y\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    611\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    612\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m estimator \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    613\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires y to be passed, but the target y is None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    614\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\Oskar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:535\u001b[0m, in \u001b[0;36mBaseEstimator._check_feature_names\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[0;32m    531\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    532\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    533\u001b[0m     )\n\u001b[1;32m--> 535\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[1;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- volume\nFeature names seen at fit time, yet now missing:\n- close\n- trading_volume\n"
     ]
    }
   ],
   "source": [
    "# Define a backtesting period\n",
    "backtest_start = \"2023-01-01\"\n",
    "backtest_end = \"2024-01-01\"\n",
    "\n",
    "# Retrieve historical data for backtesting\n",
    "backtest_data = si.get_data(ticker_symbol, start_date=backtest_start, end_date=backtest_end)\n",
    "\n",
    "if \"MA_10\" not in backtest_data.columns: # Run this block only once\n",
    "    backtest_data[\"MA_10\"] = backtest_data[\"close\"].rolling(window=10).mean()  # 10-day moving average\n",
    "    # Drop rows with NaN in 'MA_10'\n",
    "    backtest_data = backtest_data.dropna(subset=['MA_10'])\n",
    "    # backtest_data[\"Article sentiment\"] = get_ticker_news_sentiment(ticker_symbol) # fix in the future\n",
    "    backtest_data[\"Article sentiment\"] = 0 # Dummy value for now, all neutral news sentiments\n",
    "\n",
    "\n",
    "# Create future price targets for backtesting\n",
    "backtest_data[\"future_stock_price\"] = backtest_data[\"close\"].shift(-1)\n",
    "\n",
    "# Rename column\n",
    "backtest_data = backtest_data.rename(columns={\"volume\": \"trading_volume\"})\n",
    "\n",
    "\n",
    "print(backtest_data)\n",
    "\n",
    "# Drop NaN values\n",
    "backtest_features = backtest_data[[\"close\", \"trading_volume\", \"Article sentiment\", \"MA_10\"]]\n",
    "backtest_target = backtest_data[\"future_stock_price\"].dropna()\n",
    "\n",
    "# Normalize backtesting features\n",
    "backtest_features_scaled = scaler.transform(backtest_features.dropna())\n",
    "\n",
    "# Evaluate the model on backtesting data\n",
    "backtest_predictions = model.predict(backtest_features_scaled)\n",
    "\n",
    "# Calculate metrics to assess performance\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "mse = mean_squared_error(backtest_target, backtest_predictions)\n",
    "mae = mean_absolute_error(backtest_target, backtest_predictions)\n",
    "\n",
    "print(f\"Backtesting MSE: {mse}\")\n",
    "print(f\"Backtesting MAE: {mae}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: Continuous Monitoring and Model Drift Detection\n",
    "Given the dynamic nature of stock markets, it's critical to continuously monitor the model's performance to detect model drift (when the model's predictions degrade due to changing market conditions).\n",
    "\n",
    "Monitoring performance over time helps you understand when the model may need retraining or when it's experiencing drift. You might consider setting thresholds for acceptable performance levels and triggering retraining when those thresholds are exceeded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to monitor model performance over time\n",
    "def monitor_performance(model, scaler, feature_data, target_data):\n",
    "    # Normalize the feature data\n",
    "    feature_data_scaled = scaler.transform(feature_data)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = model.predict(feature_data_scaled)\n",
    "    \n",
    "    # Calculate errors\n",
    "    mse = mean_squared_error(target_data, predictions)\n",
    "    mae = mean_absolute_error(target_data, predictions)\n",
    "    \n",
    "    return mse, mae\n",
    "\n",
    "# Example: Monitor model performance with new data\n",
    "new_features = new_merged_data[[\"close\", \"trading_volume\", \"sentiment\", \"MA_10\"]].dropna()\n",
    "new_targets = new_merged_data[\"future_stock_price\"].dropna()\n",
    "\n",
    "mse, mae = monitor_performance(model, scaler, new_features, new_targets)\n",
    "\n",
    "print(f\"New data MSE: {mse}\")\n",
    "print(f\"New data MAE: {mae}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9: Deployment and Automation\n",
    "Deploying the model in a production environment requires automation to collect new data, update the model, and trigger retraining when necessary. Consider using a cloud-based environment or automated pipelines to streamline this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a pipeline for automated data collection, training, and model updating\n",
    "def automated_training_pipeline(ticker_symbol, model, scaler):\n",
    "    # Step 1: Collect new stock data\n",
    "    new_stock_data = si.get_data(ticker_symbol, start_date=datetime.now() - timedelta(days=7))\n",
    "    \n",
    "    # Step 2: Collect new news data and analyze sentiment\n",
    "    new_news_data = news.get_yf_rss(ticker_symbol)\n",
    "    new_news_sentiments = [analyze_sentiment(item[\"title\"]) for item in new_news_data]\n",
    "    new_news_df = pd.DataFrame({\n",
    "        \"timestamp\": [item[\"pubDate\"] for item in new_news_data],\n",
    "        \"sentiment\": new_news_sentiments,\n",
    "    }).resample(\"D\", on=\"timestamp\").mean()\n",
    "\n",
    "    # Step 3: Merge stock data with news sentiment\n",
    "    new_merged_data = pd.merge(\n",
    "        new_stock_data,\n",
    "        new_news_df,\n",
    "        left_on=\"date\",\n",
    "        right_index=True,\n",
    "        how=\"left\",\n",
    "    ).fillna(0)\n",
    "\n",
    "    # Step 4: Normalize features and continue training\n",
    "    new_features = new_merged_data[[\"close\", \"trading_volume\", \"sentiment\", \"MA_10\"]].dropna()\n",
    "    new_targets = new_merged_data[\"close\"].shift(-1).dropna()\n",
    "    new_features_scaled = scaler.transform(new_features)\n",
    "\n",
    "    # Step 5: Continue training the model with new data\n",
    "    model.fit(new_features_scaled, new_targets, epochs=5, batch_size=32)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pipeline demonstrates a simplified approach to automating data collection, sentiment analysis, feature engineering, and model retraining. Consider more robust infrastructure, such as containerization with Docker, cloud-based workflows, or CI/CD (Continuous Integration/Continuous Deployment) pipelines for production-ready solutions.\n",
    "\n",
    "Key Considerations\n",
    "Model Generalization: Ensure that your model does not overfit to specific patterns or short-term trends. Techniques like dropout, early stopping, and regularization can help.\n",
    "Model Drift: Regularly check for model drift and retrain when necessary.\n",
    "Data Quality and Consistency: Ensure that incoming data is accurate and consistent.\n",
    "Scalability and Robustness: Design the solution to handle large-scale data and unexpected scenarios."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
